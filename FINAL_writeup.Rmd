---
title: "Global Terrorism"
subtitle: "STAT 4185 Project Final"
author:
  - Group Four; Tom Kennon, Adam Busa, Asa Brigandi, Jiapeng Guo
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
biblio-style: datalab
output:
  bookdown::pdf_document2
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")
```

```{r setup2, echo = FALSE, message = FALSE, warning = FALSE}

## get output format in case something needs extra effort
outFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
## "latex" or "html"

## for latex output
isLatex <- identical(outFormat, "latex")
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')
```


```{r wd, echo = FALSE}
## Sets in Working Directory
setwd("C:/Users/Tom Kennon/Documents/UCONN/STAT 4185/Project Global Terrorism/Project Final Paper")

```

\newpage

# Introduction {#sec:intro}
Our project is focused on analyzing global terrorism data.  With many terrorist attacks prevalent in the news since the early 2000s as well as imposing threats of attacks being a controversial topic for a newly elected president, this an important topic at the forefront of people's minds.  The data source we used for our analysis is The Global Terrorism Database (GTD) which is an open source database that contains information about specific global terrorism incidents spanning from 1970 to 2016.   The dataset is available at: <https://www.kaggle.com/START-UMD/gtd>.  The total dataset contains 170,350 records with 135 variables.  Each record contains the date of incident, location of the incident, information about the target, information about the perpetrators etc.  Some of the variables are repetitive, do not tell provide much insight for the data, and/or are riddled with missing values as this is a real-world dataset.

Some of exploratory questions we set out to accomplish in this project are:

* What are the factors (possibly weapon type, attack type, HDI, etc.) that predict the terrorist group that perpetrated an attack?
* Investigate the trends in mass terrorism (terrorist attacks with many people killed) in the dataset.  Identify any possible unique characteristics of mass terrorism.
* What factors may predict the success rate of attacks?  What are the significant drivers for a terrorist attack to be "successful"?  Is there a bias pertaining to the recency of an attack?  [Some possible solutions: logit/probit prediction, random forest model]


# Data Wrangling {#sec:data_wrangling}

The first step to any data science project is to read in the data.  Here is the global terrorism dataset that we called: "gt" provided in a tibble format to improve readability in its data structure.
```{r data,message=FALSE,warning=FALSE}
## Reads in data
gt <- read.csv("globalterrorismdb_0617dist.csv")

library(tidyverse)

library(knitr)

## Shows dataset structure
as_tibble(gt)
```

It is often useful to include external datasets to highlight insights that may have even been unknown beforehand with just the first dataset as reference.  Our first helpful dataset is a simple gross count of total world population for each year in the dataset 1960-2016.  This is helpful for getting a better understanding of per capita statistics for each year rather than only total numbers for each year.  The second external dataset we found helpful was a Gross Domestic Product (GDP) dataset for each country in the world.  We have added the GDP of the target country from the incident year into our main dataset. The GDP data was taken from the World Bank database. We believe that GDP may be an important tool for predicting terrorist attacks.  Both of these dataset's structures are shown in tibble format below.

```{r ext_data,message=FALSE}
library("readxl")

## Reads in data
popworld <- read_excel("world population.xlsx")

## Shows dataset structure
as_tibble(popworld)


## Reads in data
gdp<-read.csv("GDP1.csv", check.names = FALSE)

## Shows dataset structure
as_tibble(gdp)
```

Now the next logical step in the data wrangling process is to join these two datasets to the global terrorism dataset which we do in this next R chunk.  We also provided code with output that shows the merges were successful.  The sample_n function is taking a random sample of 6 of the events with their years and populations.  The head function is showing the first 6 events with their country and country's respective GDPs.

```{r merge,message=FALSE}
## Joins gt data and popworld data by year as "popworld2"
popworld2 <- inner_join(popworld,gt,by=c("year"="iyear"))

## Evidence that our join was successful
set.seed(12345)
sample_n(select(popworld2, eventid, year, population),6)


## Joins gt data and gdp data by country as "master"
## a bit more complicated because the gdp dataset was less straightforward than the popworld
library("reshape2")
gdp2<-melt(gdp, id.vars = c("Country Name", "Country Code", "Indicator Name", "Indicator Code"))
gdp3<-gdp2 %>% select("Country Name", "variable", "value")
names(gdp3)[1]<-"country_txt"
names(gdp3)[2]<-"iyear"
names(gdp3)[3]<-"country_gdp"
gt$iyear<-as.character(gt$iyear)
master<-gt %>% left_join(gdp3, by = c("country_txt", "iyear"))


## Evidence that our join was successful
head(select(master, eventid, country, country_gdp))
```


# Analysis {#sec:analysis}

Interactive Leaflet Map of Global Terrorist Attacks with Useful Info. on Popup Markers

```{r leaflet, message=FALSE,eval=FALSE}
## Keeping columns of use to me
gt$date <- as.Date(with(gt, paste(imonth,iday,iyear,sep="-")), "%m-%d-%Y")

vars = gt %>% select(eventid,latitude,longitude,attacktype1_txt,
                            weaptype1_txt,target1,city,country_txt,date)

library(leaflet)
library(htmltools)

map <- leaflet(vars) %>%
  addTiles(urlTemplate = "https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png")
map %>% addMarkers(lat = ~latitude, lng = ~longitude,
        clusterOptions = markerClusterOptions(),
        popup = paste("Location:", vars$city, ",", vars$country_txt, "<br>",
                           "Date:", vars$date, "<br>",
                           "Terrorist Group:", vars$gname, "<br>",
                           "Target:", vars$target1, "<br>",
                           "Attack Type:", vars$attacktype1_txt, "<br>",
                           "Weapon:", vars$weaptype1_txt))
```

```{r mapPdf, echo = FALSE, eval = isLatex}
knitr::include_graphics(c("C:/Users/Tom Kennon/Documents/UCONN/STAT 4185/Project Global Terrorism/Project Progress Report/screenshot1.png","C:/Users/Tom Kennon/Documents/UCONN/STAT 4185/Project Global Terrorism/Project Progress Report/screenshot2.png","C:/Users/Tom Kennon/Documents/UCONN/STAT 4185/Project Global Terrorism/Project Progress Report/screenshot3.png","C:/Users/Tom Kennon/Documents/UCONN/STAT 4185/Project Global Terrorism/Project Progress Report/screenshot4.png","C:/Users/Tom Kennon/Documents/UCONN/STAT 4185/Project Global Terrorism/Project Progress Report/screenshot5.png"))
```


Above is an interactive geospatial visualization showing all of the terrorist attacks in this database.  Because this is an html document, you can use the interactive map to find whatever terrorist attacks that interest you.  When a popup is chosen, the location, date, terrorist group, target name, attack type, and weapon type are displayed.  This map highlights where many terrorist attacks are.  The Middle East area has a lot of attacks.  To see a more detailed number for each country or region, see the following two outputs.


Rank of Number of Attacks for each Country with Descending Order.

```{r countryattacks,message=FALSE}
## Rank of number of attacks for each country with descending order. 
gt %>%
  group_by(country_txt) %>%
  summarise( nr_of_attacks = n()) %>%
  arrange(desc(nr_of_attacks)) %>%
  head(n=10)
```
As evidenced by the output above, Iraq has the most number of attacks, followed by Pakistan, Afghanistan, etc.  These countries have been known to have had a lot of terrorist attacks and this is backed in our findings as well.


Rank of Number of Attacks for each Region with Descending Order.

```{r regionattacks,message=FALSE}
## Rank by region
gt %>%
  group_by(region_txt) %>%
  summarise(attacks = n()) %>%
  arrange(desc(attacks)) %>%
  head(n=10)
```
As evidenced by the output above, the Middle East and North Africa and South Asia have the most number of attacks which has been echoed in world news lately due to shifting political regimes.

Wordcloud of Summaries of Global Terrorist Attacks Since 1970

```{r wordcloud, message=FALSE}
library(tm)
library(SnowballC)
library(wordcloud)

corpus <- Corpus(VectorSource(gt$summary))
corpus <- tm_map(corpus,removePunctuation)
corpus <- tm_map(corpus,removeWords,c("the","this",stopwords("english")))
corpus <- tm_map(corpus,stemDocument)

wordcloud(corpus,max.words=100,random.order=FALSE)

```

The above plot shows a wordcloud visualization of detailed summaries of these terrorist attacks.  The most frequent word used in the descriptive summaries for these attacks is "response", followed by "attack", "claim", and "group".  This is a good way to visualize what is important to know while investigating terrorist attacks so this aids us in our analysis.



(ref:cap-atype) Breakdown of Attack Type.

```{r atype,message=FALSE,fig.cap = "(ref:cap-atype)", fig.width = 8}
################
### Figure 1 ###
################


## Bar chart
ggplot(gt,aes(attacktype1)) +
  geom_bar(aes(fill=attacktype1_txt)) +
  labs(title="Attack Types", y="Count", x="") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  guides(fill=guide_legend(title="Attack Type"))

```
In Figure \@ref(fig:atype), we can see that the most popular attack type in terrorist attacks since 1970 are bombings/explosions followed by armed assaults and assassinations respectively.  But how do these numbers change over time?


(ref:cap-ayear1) Scatterplot: Trends in Attack Types Since 1970.

```{r ayear1,message=FALSE,fig.cap = "(ref:cap-ayear1)", fig.width = 8}
################
### Figure 2 ###
################

cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999")

y<-as.data.frame(table(gt$iyear, gt$attacktype1_txt))
colnames(y) <- c("iyear", "attacktype1_txt", "freq")
ggplot(data=y) +
  geom_point(mapping = aes(x=iyear, y=freq, color=attacktype1_txt))+
  scale_color_manual(values=cbbPalette) +
  scale_x_discrete(breaks=c(1970,1980,1990,2000,2010))
```


(ref:cap-ayear2) Bubblechart: Trends in Attack Types Since 1970.

```{r ayear2,message=FALSE,fig.cap = "(ref:cap-ayear2)", fig.width = 8}
################
### Figure 3 ###
################

ggplot(data=gt) +
  geom_count(mapping = aes(x=iyear, y=attacktype1_txt)) +
  scale_x_discrete(breaks=c(1970,1980,1990,2000,2010))
```
In Figure \@ref(fig:ayear2) and Figure \@ref(fig:ayear1), we have created 2 different graphs that illustrate trends in attack types since 1970. In it we can clearly see there has been a large spike in bombings in recent years, as well as a mild spike in armed assault.  While most other methods have seen an increase in recent years, these two are by far the greatest increase. This may imply that these are simply the preferred methods by groups that started up since the late 1990’s/early 2000’s. It could also mean that terrorist groups have just experienced more success with these methods, and thus have been utilizing them more. We have the necessary data to test both of these possibilities.  Another interesting variable to look into is the terrorist group.


(ref:cap-group) Successful Terrorist Groups

```{r group,message=FALSE,fig.cap = "(ref:cap-group)", fig.width = 8}
################
### Figure 4 ###
################


## Identify major terrorist organizations
terr.groups <- gt %>% 
  select(gname, success) %>% 
  filter(!gname %in% "Unknown") %>% 
  group_by(gname) %>% 
  summarise(n.success=sum(success)) %>% 
  arrange(desc(n.success)) %>% 
  filter(n.success >= 1500) #this will filter out any terrorist groups with less than 1500 successful attacks

terr.groups$n.success <- as.numeric(terr.groups$n.success)

## Bar chart
ggplot(terr.groups, aes(gname, n.success)) +
  geom_bar(stat="identity",aes(fill=gname)) +
  labs(title="Successful Attacks by Group Name", y="Successes", x="") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  guides(fill=guide_legend(title="Group Name"))

```
In Figure \@ref(fig:group) we can see the which terrorist groups have the most successful attacks.  The Taliban has the most successful attacks at approximately 6000 and the Shining Path (SL) and Islamic State of Iraq and the Levant (ISIL) have the second and third most respectively.

(ref:cap-group2) Successful Terrorist Groups Over Time

```{r group2,message=FALSE,fig.cap = "(ref:cap-group2)", fig.width = 8}
################
### Figure 5 ###
################

## Over time
gname.line <- gt %>% 
  select(gname,iyear,success) %>% 
  group_by(gname,iyear) %>% 
  summarise(n.success = sum(success)) %>% 
  filter(gname %in% c("Al-Shabaab","Basque Fatherland and Freedom (ETA)", "Boko Haram", "Communist Party of India - Maoist (CPI-Maoist)", "Farabundo Marti National Liberation Front (FMLN)", "Irish Republican Army (IRA)", "Islamic State of Iraq and the Levant (ISIL)", "Kurdistan Workers' Party (PKK)", "Liberation Tigers of Tamil Eelam (LTTE)", "New People's Army (NPA)", "Revolutionary Armed Forces of Columbia (FARC)", "Shining Path (SL)", "Taliban"))

gname.line$iyear <- as.numeric(gname.line$iyear)

ggplot(gname.line,aes(iyear, n.success)) +
  geom_line(aes(color=gname)) +
  labs(title="Terrorist Group Activity Since 1970", y="Successes", x="") +
  guides(fill=guide_legend(title="Group Name")) +
  scale_x_discrete(breaks=c(1970,1980,1990,2000,2010))

```
In Figure \@ref(fig:group2) we can see the which terrorist groups have the most successful attacks over time.  This is a useful insight that adds to Figure \@ref(fig:group) because now it is evident which groups have had an influx in attacks as of late compared to groups that have died out.  Al-Shabaab has only had attacks since 2007, while the Irish Republican Army used to have a lot of terrorist attacks but not as many as of recently.  The Islamic State of Iraq and the Levant (ISIL)'s terrorist attacks have only had attacks as of 2013 yet have the third most successful terrorist attacks in data back to 1970 so they are a scary threat.



(ref:cap-attacksovertime) Plot of Terrorist Attacks Over Time.

```{r attacksovertime,message=FALSE,warning=FALSE,fig.cap = "(ref:cap-attacksovertime)", fig.width = 8}
################
### Figure 6 ###
################


## number of terrorist attacks over time
ggplot(gt, aes(x=iyear)) +
  geom_histogram(stat='count') +
  labs(title='Terrorism attacks over time') +
  scale_x_discrete(breaks=c(1970,1980,1990,2000,2010))
```


(ref:cap-terrvspop) Plot of Terrorism Growth vs. World Population Growth Over Time

```{r terrvspop,message=FALSE,fig.cap = "(ref:cap-terrvspop)", fig.width = 8}
################
### Figure 7 ###
################

## plot terrorism growth versus population growth
p1 <- ggplot(data=popworld2, aes(x=year)) +
  geom_histogram(aes(col='Attack Count'),bins=45) + 
  scale_x_continuous(breaks=seq(1970, 2016, 10))

p1+ geom_line(aes(y=population/500000, col='Population')) + 
    scale_y_continuous(sec.axis = sec_axis(~ . * 500000, name = "Population Size"))+
    labs(y = "Attack Count", x = "Year", colour = "Blue") +
    labs(title="Terrorism growth vs Population growth")
```
As evidenced by Figure \@ref(fig:attacksovertime), there is an increase in terrorist attacks as the year increases.  However, there is also an increase in the general world population as we know from our merge with the world population dataset.  One can see from viewing Figure \@ref(fig:terrvspop), that there is still a rapid increase in terrorist attacks even accounting for world population size aka there is still an increasing amount of terrorist attacks per capita as the years increase which relates to one of our main questions.





(ref:cap-yearkillings) Worldwide Killings Grouped by Year.

```{r yearkillings,message=FALSE,fig.cap = "(ref:cap-yearkillings)", fig.width = 8}
################
### Figure 8 ###
################

## Worldwide killings grouped by year
datakill <- gt %>% filter(nkill>0)
data_1<-datakill %>% group_by(iyear) %>% summarise(n=sum(nkill))
colnames(data_1)<-c("Year","Killed")
data_1$Year <- as.numeric(data_1$Year)
data_1$Killed <- as.numeric(data_1$Killed)
p2<-ggplot(data_1,aes(x=Year,y=Killed))+
  geom_line()+theme_bw()
p2

```
As evidenced by Figure \@ref(fig:yearkillings), there is an increasing trend of number of people killed after 2000 and a very high increasing trend after 2010.  This relates to one of main questions if there is an increase in terrorist attacks as of recently indicating that there is not just a recency bias!  More people are being killed in terrorist attacks as the years go on.


# Predictive Analytics

## Predicting "ISIL" Terrorist Group

### Naive Bayes

Here we lay out our methodology for the Naive Bayes Model.

```{r isilcode,eval=TRUE,echo=FALSE,message=FALSE,warning=FALSE}
library(dplyr)
library(e1071)
library(mdsr)

## Reads in data
gt <- read.csv("globalterrorismdb_0617dist.csv")

library(tidyverse)

## Shows dataset structure
as_tibble(gt)

terror <- gt
#First I will try and create a naive Bayes model to predict whether an attack was carried out by ISIL.

terror2<-filter(terror, iyear>=2013)%>% 
  mutate("Isil" = ifelse(gname=="Islamic State of Iraq and the Levant (ISIL)", 1, 0))
terror2$Isil<-as.factor(terror2$Isil)
#Here, I'm creating a binary variable for whether an attack is known to have been carried out by ISIL.
#I limited the data to 2013-2016 as those are the years Isil has been most active


set.seed(364)
n<-nrow(terror2)
test_id<-sample.int(n, size=round(0.2*n))

train<-terror2[-test_id,]
#nrow(terror2)

test<-terror2[test_id,]
#nrow(test)
#perform an 80/20 split into a training and testing set

#tally(~Isil, data=train, format="percent")
#creating a null model, shows 7.5 percent of attacks are known to be carried out by ISIL


form<-as.formula("Isil~iyear+region_txt+attacktype1_txt+targtype1_txt+weaptype1_txt")
#creating a model using the country, attack type, target type, and weapon type as predictors

mod_NB_train<-naiveBayes(form, data=train)
#mod_NB_train
pred_NB_train<-predict(mod_NB_train, newdata=train)
#pred_NB_train

confusion_train <- table(pred_NB_train, train$success)
#confusion_train

Accuracy_train <-sum(diag(confusion_train)) / nrow(train)
#Accuracy_train
#My training set appears to have been predicted with about 16.46 percent accuracy


mod_NB_test<-naiveBayes(form, data=test)
pred_NB_test<-predict(mod_NB_test, newdata=test)
#pred_NB

confusion_test <- table(pred_NB_test, test$success)
#confusion_test


Accuracy_test <-sum(diag(confusion_test))   / nrow(test)
#Accuracy_test
#The test set was predicted with about 15.33 percent accuracy

#Overall, the model does not appear to be overfitted, and this is an improvement from the null model

```

```{r isilform,echo=TRUE}
tally(~Isil, data=train, format="percent")
## Creating a null model, shows 7.5 % of attacks are known to be carried out by ISIL

form<-as.formula("Isil~iyear+region_txt+attacktype1_txt+targtype1_txt+weaptype1_txt")
## Creating a model using the country, attack type, target type,
##  and weapon type as predictors
```

Our null model shows that 7.5% of terrorist attacks are to be carried out by ISIL, so this is the model to beat.  We also write our formula to use for the Naive Bayes modelling.  We use ISIL (coded 0 for non-ISIL terrorist group and 1 for ISIL terrorist group) as the dependent variable.  The predictors we threw into the model include year the attack happened, region of the world where the attack occurred, the attack type, the target type, and the weapon type.




Here is our Naive Bayes model fitting and our corresponding predictions for the training and testing set.

```{r isilmods,echo=TRUE,eval=FALSE}
mod_NB_train<-naiveBayes(form, data=train)
## Naive Bayes model for Training Data

pred_NB_train<-predict(mod_NB_train, newdata=train)
## Prediction using the Naive Bayes model for Training Data

mod_NB_test<-naiveBayes(form, data=test)
## Naive Bayes model for Testing Data

pred_NB_test<-predict(mod_NB_test, newdata=test)
## Prediction using the Naive Bayes model for Testing Data
```


```{r isilmats,eval=TRUE,echo=TRUE,message=FALSE,warning=FALSE}
confusion_train <- table(pred_NB_train, train$success)
confusion_train
## Confusion Matrix for Training Set

Accuracy_train <-sum(diag(confusion_train))   / nrow(train)
Accuracy_train
## Our training set appears to have been predicted with about 16.46 percent accuracy


confusion_test <- table(pred_NB_test, test$success)
confusion_test
## Confusion Matrix for Test set


Accuracy_test <-sum(diag(confusion_test))   / nrow(test)
Accuracy_test
## The test set was predicted with about 15.33 percent accuracy
## Overall, the model does not appear to be overfitted, and this is an improvement from the null model
```

Here we are trying to predict whether a terrorist attack was carried out by ISIL, utilizing weapon type, target type, attack type, and country as predictors. To do this effectively, we limit the data to only the years 2013-2016, as these are the years when ISIL became more active in their attacks (as seen in our descriptive analysis seen in Figure \@ref(fig:group2)). Our null model showed that if we assume all attacks are committed by ISIL, we are only right about 7.5 percent of the time. However, we were able to correctly determine if an attack was carried out by ISIL with 17.67 percent accuracy in our training set, and with about 17.21 percent accuracy in our testing set. These accuracies are a decent improvement over the null model, and are close enough that we are not concerned with overfitting.  In this example we have laid out the general framework for a Naive Bayes classification so in our future analysis discussing Naive Bayes we will only provide and interpret the output rather than show all of our code.


## Predicting Mass Terrorist Attacks (>3 Deaths)

In this section, we show which predictors could lead to mass terrorist attacks.  We want to investigate mass terrorism in our dataset. We define a "mass terrorist attack" as an attack that killed more than three people (3 kills is the 85th percentile of the *nkill* variable in our dataset).  We drop any missing values in the *nkill* variable.

```{r mass,eval=TRUE,echo=FALSE,message=FALSE,warning=FALSE}
library(rpart)
library(partykit)
library(randomForest)
library(rpart.plot)
library(pROC)


#drop missing value from nkill and create a new data set
data_no_na <- gt %>% drop_na(nkill)

#find quantile in nkill and assign the number larger than 3 is a mass munder. the 85% is 3 so I assigned the number is larger than 3 which is defined a mass terrirsom 
quantile(data_no_na$nkill,probs = c(0,0.25,0.5,0.75,0.85,0.90,1))

#create dummy variables 0 for equal and less than 3 and 1 for larger than 3
massTerr <- as.numeric(data_no_na$nkill>3)
names(massTerr)<-"massTerr" # name this new variable 
data1<-cbind(massTerr,data_no_na)#new data frame with dummy variables
```


### Decision Tree

```{r masstree01,eval=TRUE,echo=FALSE}
decision_tree<-select(data1,massTerr,attacktype1_txt,iyear,targtype1_txt,weaptype1_txt,extended,suicide,region_txt)
## Selecting all the predictors and creating a new data set called "decision tree"

set.seed(69420)
n  <- nrow(decision_tree)
test_idx <- sample.int(n, size = round(0.2*n)) #select the size of testing data
train <- decision_tree[-test_idx, ]
#nrow(train)
test <- decision_tree[test_idx, ]
#nrow(test)
## Data Partition

```


```{r masstree,eval=TRUE,echo=TRUE}

tally(~massTerr, data=train,format="percent") #null model has 85.62% accuracy
## Null model

   
mod_tree <- rpart(massTerr ~ ., data = train, method = "class",control = rpart.control(cp = 0.01))
#mod_tree
## Builds Decision Tree 
```

(ref:cap-masstreeplot) Decision Tree Plot.

```{r masstreeplot,eval=TRUE,echo=TRUE,fig.cap = "(ref:cap-masstreeplot)", fig.width = 8}
rpart.plot(mod_tree)
## Plots Decision Tree  
```


Our null model shows that 85.62% of terrorist attacks have 3 or fewer kills.  We also build our decision tree here.  We use massTerr (coded 0 for 3 or fewer kills in the terrorist attack and 1 for greater than 3 kills in the terrorist attack) as the dependent variable.  The predictors we threw into the model include the attack type, the year the attack happened, the target type, the weapon type, extended, if there was a suicide attempt, and region of the world where the attack occurred.

Based on the plot of this decision tree in Figure \@ref(fig:masstreeplot), the most important factor used in predicting a "mass terrorist attack" was whether or not it was a suicide attack. If the attack was not a suicide attack (representing about 96% of the data), the model decides that the attack was not a mass terrorist attack. If the attack *was* a suicide (representing about 4% of the data), the model then looks to the "target type" variable to determine whether the attack killed more than three people. If the suicide attack was directed toward some maritime, telecommunication, or some unknown target, the model decides that the attack was not a mass terrorist attacks. If the suicide attack was not directed at one of the aforementioned targets, the model decides that the attack was a mass terrorist attack.  

```{r massacc,echo=TRUE,eval=TRUE}
train.dt <- train %>%
  mutate(massTerr_dtree   = predict(mod_tree,type="class",data=train))
confusion.mass.terr <- tally(massTerr_dtree   ~ massTerr,data=train.dt,format="count")
confusion.mass.terr
## Confusion Matrix Training



dtAcc.mt <- sum(diag(confusion.mass.terr))  / nrow(train)
dtAcc.mt
## Accuracy for Training Data


test.dt <- test %>%
  mutate(massTerr_dtree   = predict(mod_tree, newdata=test, type="class"))
confusion.mass.terr_test <- tally(massTerr_dtree   ~ massTerr, data = test.dt, format = "count")
confusion.mass.terr_test
## Confusion Matrix Testing


dtAcc.mt_test <- sum(diag(confusion.mass.terr_test))  / nrow(test)
dtAcc.mt_test
## Accuracy for Testing Data


DT.mt <- c(dtAcc.mt,dtAcc.mt_test)
dtAcc.mt ## Accuracy of Training
dtAcc.mt_test ## Accuracy of Testing
```

Our null model shows that 85.62% of terrorist attacks have 3 or fewer kills.  However, we were able to correctly determine if an attack was a mass terrorist attack with 86.42% accuracy in our training set, and with about 86.49% percent accuracy in our testing set. These accuracies are a slight improvement over the null model, and are close enough that we are not concerned with overfitting.  Honestly, the testing set had a higher accuracy than the training, so we may have been able to include even more and still be okay!  In this example we have laid out the general framework for a decision tree classification so in our future analysis discussing decision trees we will only provide and interpret the output rather than show all of our code.


### Random Forest Model

```{r randomforestmassterr,warning=FALSE,message=FALSE}
##################### To make Random Forest
mod_forest.mt <- randomForest(as.factor(massTerr) ~ . ,data=train,ntree=200,mtry=3)
mod_forest.mt
## Modeling Training data







forestAcc.mt <- sum(diag(mod_forest.mt$confusion)) / nrow(train)
forestAcc.mt ## 0.8733331
## Accuracy of Training


imp.mt <- importance(mod_forest.mt)    %>%
     as.data.frame()   %>%
     rownames_to_column()    %>%
     arrange(desc(MeanDecreaseGini))

imp.mt
## Importance Scores



mod_forest.mt_test <- randomForest(as.factor(massTerr) ~ ., data = test, ntree = 200, mtry = 3)
mod_forest.mt_test
## Modeling Test data


forestAcc.mt_test <- sum(diag(mod_forest.mt_test$confusion)) / nrow(test)
forestAcc.mt_test ## 0.8716002
## Accuracy of Testing

Forest.mt <- c(forestAcc.mt,forestAcc.mt_test)
## the training accuracy is 0.8733331 0.8716002


```

Based on the random forest model the most three important predictors are which year, target type and whether is a suicide attack or not.  
The training accuracy is 0.873 and the testing accuracy is 0.872. So we can find that this two number is close which means our model is not overfitting. 

### Model Selection

(ref:modelCompareJiapeng) Mass Terrorism Model Accuracy Comparison

```{r modelCompareJiapeng,echo=FALSE}
Label <- c("Train","Test")
nullAcc.mt <- table(train$massTerr)[[1]]/nrow(train)
nullAcc.mt_test <- table(test$massTerr)[[1]]/nrow(test)
Null.mt <- c(nullAcc.mt,nullAcc.mt_test)

modelCompare.mt <- data.frame(Data=Label,Null=Null.mt,"Decision Tree"=DT.mt,"Random Forest"=Forest.mt)
#modelCompare.mt

knitr::kable(modelCompare.mt,caption = '(ref:modelCompareJiapeng)', booktabs=TRUE,align="c")
```


Based on Table \@ref(tab:modelCompareJiapeng), we can conclude that the Random Forest model is the best predictor of mass terrorism attacks. This said, it is important to note that neither the Decision Tree nor the Random Forest model improved on the null by more than 2% in accuracy. It is possible that the accuracies of these models could be improved through more intensive model specifications, but optimizing the models was not the goal of this investigation.  In this example we have laid out the general framework for a random forest classification so in our future analysis discussing random forest we will only provide and interpret the output rather than show all of our code.



## Predicting "Successful" Terrorist Attacks

```{r partsucc, echo=FALSE}
#partition data
set.seed(69420)
 n  <- nrow(gt)
 test_idx <- sample.int(n, size = round(0.2*n))
 
#train
 train <- gt[-test_idx, ]

#test
 test <- gt[test_idx, ]
```

In the dataset, success is a logical variable describing the "success" of an attack. "Success" is defined differently for different types of attacks (i.e. the success of a kidnapping is defined differently than the success of a bombing). Our goal is to build a model that can predict the success of an attack given information provided in the data. We will approach this question using four models:

- Naive Bayes
- Decision Tree
- Random Forest
- Logistic Regression

Since only about 10.25% of the incidents in the training data were not successful, the accuracy of the null model is already 89.75% just by assuming all attacks are successful.

```{r null,echo=TRUE}
#null model
tally(~success, data = train, format = "percent")
```


### Naive Bayes

```{r nbsucc,echo=FALSE}
form2<-as.formula("as.factor(success)~iyear+region_txt+attacktype1_txt+targtype1_txt+weaptype1_txt")
#creating a model using the country, attack type, target type, and weapon type as predictors 

print("form2")
form2


mod_NB2<-naiveBayes(form2, data=train)
pred_NB2<-predict(mod_NB2, newdata=train)

confusionNB <- table(pred_NB2, train$success)
print("confusionNB")
confusionNB

NBAcc<-sum(diag(confusionNB))   / nrow(train)
print("NBAcc")
NBAcc
#My training set appears to have been predicted with about 90.51 percent accuracy


mod_NB2_test<-naiveBayes(form2, data=test)
pred_NB2_test<-predict(mod_NB2, newdata=test)

confusionNB_test <- table(pred_NB2_test, test$success)
print("confusionNB_test")
confusionNB_test

NBAcc_test<-sum(diag(confusionNB_test))   / nrow(test)
print("NBAcc_test")
NBAcc_test
#The test set was predicted with about 89.97 percent accuracy
#Overall, the model does not appear to be overfitted and is a slight improvement from the null model

NB <- c(NBAcc,NBAcc_test)


```

Here, we try to predict the success of a terrorist attack, utilizing year, region, weapon type, target type, and attack type as predictors. Our null model showed that if we assume all attacks are successful, we are only right 89.75% of the time. However, we were able to correctly determine successful attack with 90.51% accuracy in our training set, and with about 89.97% accuracy in our testing set. These accuracies are an improvement over the null model, and they are close enough that we are not concerned with overfitting. However, these are still very inaccurate. Thus, while this is an improvement from the null model, we cannot recommend utilizing this model to determine if an attack was successful. 


### Decision Tree

```{r dtsucc,echo=FALSE}
#formula
form <- as.formula("as.factor(success)~iyear+attacktype1_txt+targtype1_txt+weaptype1_txt")

print("form")
form


#fitted model 
mod_tree1 <- rpart(form, data=train)
#mod_tree1
```

(ref:cap-succdtplot) Decision Tree for Attack Success

```{r succdtplot,eval=TRUE,echo=FALSE,fig.cap = "(ref:cap-masstreeplot)", fig.width = 8}
#plot decision tree
rpart.plot(mod_tree1)
```

Figure \@ref(fig:succdtplot) is a decision tree that identifies target type as the most important predictor in the model. If the target type is unknown for a particular attack (representing 3% of the data), the model uses the year of the attack to further classify the attack as either a success or a failure. If the target type is known (representing 97% of the data), the model then uses attacktype to further classify the observation. If the target type is known and the attack type was not an assassination, the model classifies the attack as a success. If the target type is known and the attack type was and assassination, the model then looks to the weapon type to determine whether the attack was successful. 

```{r succconfus,echo=FALSE}
#confusion matrix
train.dt <- train %>%
   mutate(success_dtree   = predict(mod_tree1, type="class"))
 confusion.dt <- tally(success_dtree   ~ success, data = train.dt, format = "count")
 
print("Confusion matrix training")
confusion.dt
 

dtAcc <- sum(diag(confusion.dt))  / nrow(train)


#overfit?
test.dt <- test %>%
   mutate(success_dtree   = predict(mod_tree1, newdata=test, type="class"))
 confusion.dt_test <- tally(success_dtree   ~ success, data = test.dt, format = "count")

print("Confusion matrix testing")
confusion.dt_test

 dtAcc_test <- sum(diag(confusion.dt_test))  / nrow(test)

DT <- c(dtAcc,dtAcc_test)

print("Accuracy Training")
dtAcc

print("Accuracy Testing")
dtAcc_test

```

Despite this attempt to intuitively choose predictors for a decision tree, its rules did not sufficiently improve on the accuracy of the null model. Choosing predictors based on intuition can lead to dead ends, and perhaps the importance of predictors should be left to a random forest model.


### Random Forest Model

```{r rfsuccout, echo=TRUE, eval=FALSE}
#Call:
# randomForest(formula = form, data = train, ntree = 200, mtry = 3) 
#               Type of random forest: classification
#                     Number of trees: 200
#No. of variables tried at each split: 3
#
#        OOB estimate of  error rate: 8.93
#Confusion matrix:
#     0      1 class.error
#0 3513  10454  0.74847856
#1 1720 120593  0.01406228
```

(ref:succimprf) Attack Success: Random Forest Predictor Importance

```{r succimprf, echo=FALSE}

Forest <- c(0.9106692,0.9039331) 

### ### ### ### ### ### ### ### ### ### ###

#write.csv(imp.rf, "imp_rf.csv")
imp.rf <- read.csv("imp_rf.csv")
imp.rf <- imp.rf[,-1]
knitr::kable(imp.rf,caption = '(ref:succimprf)', booktabs=TRUE,align="c")

### ### ### ### ### ### ### ### ### ### ###
```


```{r succimprfacc, echo=FALSE}
print("Accuracy of Training")
print(0.9106692)
print("Accuracy of Testing")
print(0.9039331)
```


Table \@ref(tab:succimprf) according to the Random Forest model identifies "target type," "country," and "attack type" as the most important variables for predicting the success of a terrorist attack. The Random Forest model was able to improve on the accuracy of the decision tree model, and it is safe to assume that the model was not overfitted to the train data based on the similarity in accuracy between the train and test data. This said, neither of the tree models were really able to noticeably (>2% better) improve on the 89.75% null model accuracy.

### Logistic Regression

```{r logreg, echo=TRUE}
logit1 <- glm(success ~ iyear + extended + suicide + targtype1_txt, family = binomial(logit), data = train)
summary(logit1)

predLogit <- predict(logit1, newdata=train, type=c("response"))

#confusion matrix
confusion.Logit <- table(predLogit > 0.5,train$success)
confusion.Logit

#accuracy
logitAcc <- sum(diag(confusion.Logit)) / nrow(train)

#overfitting?
predLogit_test <- predict(logit1, newdata=test)
confusion.Logit_test <- table(predLogit_test > 0.5,test$success)
logitAcc_test <- sum(diag(confusion.Logit_test)) / nrow(test)

Logit <- c(logitAcc,logitAcc_test)
Logit

#ROC Curve
roccurve <- roc(test$success,predLogit_test)
plot(roccurve)
```

The model does not seem to be overfit, seeing that the accuracy of the model on both the train and test data are very similar. This model yielded an accuracy of 90.45% for the training data set and an accuracy of 89.84% for the test data set. The accuracy of this model is still unable to predict attack success effectively better than just assuming all attacks are successful.  The ROC curve shown is an indication that the model is predicting success better than random chance (since it lies well above the 45 degree line). While this is not of much use for a single model other than to show its general suitability, it is helpful when comparing logistic regression models with different predictors.


### Model Selection

(ref:modelCompare) Attack Success: Model Accuracy Comparison Table

```{r modelCompare}
Label <- c("Train","Test")
nullAcc <- table(train$success)[[2]]/nrow(train)
nullAcc_test <- table(test$success)[[2]]/nrow(test)
Null <- c(nullAcc,nullAcc_test)

modelCompare <- data.frame("Data"=Label,Null,"Naive Bayes"=NB,"Decision Tree"=DT,"Random Forest"=Forest,Logit)
knitr::kable(modelCompare,caption = '(ref:modelCompare)', booktabs=TRUE,align="c")
```

Table \@ref(tab:modelCompare) shows the accuracies of these 4 different modeling techniques on the predicting of the success rate.  So according to accuracy, the random forest, although computationally expensive appeared to perform the best.  The null model appeared to perform the worst (which means that our models were at least somewhat helpful!).

# Conclusion {#sec:conclusion}

To recap, some of our exploratory questions we set out to accomplish in this project are:

* What are the factors (possibly weapon type, attack type, HDI, etc.) that predict the terrorist group that perpetrated an attack?
* Investigate the trends in mass terrorism (terrorist attacks with many people killed) in the dataset.  Identify any possible unique characteristics of mass terrorism.
* What factors may predict the success rate of attacks?  What are the significant drivers for a terrorist attack to be "successful"?  Is there a bias pertaining to the recency of an attack?  [Some possible solutions: logit/probit prediction, random forest model]



We know from our descriptive analysis, that terrorist attacks, as well as number of people killed in terrorist attacks, are increasing worldwide.  Even accounting for the world's growing population, the number of attacks are still rising per capita at an alarming rate.  So thus, there is not just a recency bias as some may have speculated.  We will seek to model and interpret why this could be later in our project.  There have also been a lot of terrorist attacks in the Middle East and South Asia.  This fact is highly known in current events as there is a lot of political turmoil in many of those nations.  We have also seen that there has been a large increase in bombings in recent years, as well as a mild spike in armed assault both of which also have the highest gross total of attacks since 1970.  Since the late 1990’s/early 2000’s, this increase has been seen.  Why is this so?  This is a question we wish to further explore in our predictive analysis in the future; is this increase because terrorist attacks of this type are more successful?  We also observed that terrorist groups like Islamic State of Iraq and the Levant and Al-Shabaab are on the rise recently, while the Taliban has the most successful attacks since 1970.


For predicting if the terrorist group was "ISIL" or not using a Naive Bayes model, our null model showed that if we assume all attacks are committed by ISIL, we are only right about 7.5 percent of the time. However, we were able to correctly determine if an attack was carried out by ISIL with 17.67 percent accuracy in our training set, and with about 17.21 percent accuracy in our testing set.  So we are decently better at predicting by accuracy than the null model.

For predicting if the terrorist attack was a "mass attack" or not using a Decision tree and Random Forest, we saw in Table \@ref(tab:modelCompareJiapeng), we could conclude that the Random Forest model was the best predictor of mass terrorism attacks. This said, it is important to note that neither the Decision Tree nor the Random Forest model improved on the null by more than 2% in accuracy.

For predicting if a terrorist attack was a success, we produced 4 different prediction models, including Naive Bayes, Decision Tree, Random Forest, and even a Logistic Regression, with the Random Forest performing the best and all of our models beating the null.  Table \@ref(tab:modelCompare) can show more detail.  We feel we have accomplished the goals and exploratory questions we originally set out to accomplish in this analysis.


